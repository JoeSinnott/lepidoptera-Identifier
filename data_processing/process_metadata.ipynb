{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac545b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import os\n",
    "\n",
    "# --- Dask Setup ---\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b32da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and Process Taxonomy Lookup Table ---\n",
    "\n",
    "with open('taxon_ids.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "taxon_ids = []\n",
    "taxon_names = []\n",
    "\n",
    "for line in lines:\n",
    "    split_pos = line.find(' ')\n",
    "    if split_pos != -1:\n",
    "        taxon_ids.append(int(line[:split_pos]))\n",
    "        taxon_names.append(line[split_pos+1:].strip())\n",
    "\n",
    "# Create a Pandas DataFrame for the taxonomy data\n",
    "df_taxa = pd.DataFrame({'taxon_id': taxon_ids, 'name': taxon_names})\n",
    "df_taxa['taxon_id'] = df_taxa['taxon_id'].astype('float64') # Cast to float to match the observations data type\n",
    "\n",
    "df_taxa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e0744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Lazily Load Large Datasets ---\n",
    "\n",
    "ddf_obs = dd.read_csv('observations.csv',\n",
    "                      sep='\\t',\n",
    "                      usecols=['observation_uuid','taxon_id'],\n",
    "                      dtype={'taxon_id': 'float64'},\n",
    "                      blocksize='50MB')\n",
    "\n",
    "ddf_photos = dd.read_csv('photos.csv',\n",
    "                         sep='\\t',\n",
    "                         usecols=['observation_uuid','photo_id', 'extension'],\n",
    "                         blocksize='50MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471a9592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Combine and Clean DataFrames ---\n",
    "\n",
    "ddf_merged = dd.merge(ddf_obs, ddf_photos, on='observation_uuid', how='inner')\n",
    "\n",
    "ddf_merged = dd.merge(ddf_merged, df_taxa, on='taxon_id', how='left')\n",
    "\n",
    "ddf_merged = ddf_merged.dropna(subset=['name'])\n",
    "\n",
    "ddf_merged['taxon_id'] = ddf_merged['taxon_id'].astype('int64')\n",
    "\n",
    "ddf_merged = ddf_merged.drop(columns=['observation_uuid', 'taxon_id'])\n",
    "\n",
    "\n",
    "with open('top100species.txt', 'r') as f:\n",
    "    top_100_species_list = eval(f.read())\n",
    "\n",
    "# Filter the original DataFrame to keep only rows with names in the top species list\n",
    "filtered_ddf = ddf_merged[ddf_merged['name'].isin(top_100_species_list)]\n",
    "\n",
    "print('Dataframe creation complete.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3223b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Execute Computation and Save to Parquet ---\n",
    "\n",
    "print(\"Starting computation and saving to Parquet file...\")\n",
    "\n",
    "# This command triggers the full Dask computation graph\n",
    "filtered_ddf.to_parquet('photo_id_data.parquet', engine='pyarrow', write_index=False)\n",
    "\n",
    "print(\"\\nProcessing complete! File 'photo_id_data.parquet' has been saved.\")\n",
    "\n",
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
